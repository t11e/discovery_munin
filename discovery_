#!/usr/bin/env python
#%# family=auto
#%# capabilities=autoconf suggest

import commands
import math
import os
import re
import sys
import urllib
import urlparse
import xml.dom.minidom

from optparse import OptionParser

def getDirSize(path):
  size = 0
  for f in os.listdir(path):
    if os.path.isdir(os.path.join(path, f)):
      size += getDirSize(os.path.join(path, f))
    else:
      size += os.path.getsize(os.path.join(path, f))
  return size

def readFile(path, openFn = open):
  output = None
  try:
    f = openFn(path)
  except IOError:
    f = None
  if f is not None:
    try:
      output = f.read()
    finally:
      f.close()
  return output

def readStats(url, path):
  return readProperties(urlparse.urljoin(url, path), ':', urllib.urlopen)

def readProperties(path, separator, openFn = open):
  output = None
  try:
    f = openFn(path)
  except IOError:
    f = None
  if f is not None:
    try:
      output = {}
      for line in f:
        line = line.strip()
        if len(line) == 0 or line.startswith("#"): continue
        try:
          key, value = map(lambda s: s.strip(), line.split(separator, 1))
          output[key] = value
        except ValueError:
          pass
    finally:
      f.close()
  return output

def findEngineDirs(searchPath):
  for path in searchPath.split(os.path.pathsep):
    if len(path) == 0: continue
    f = os.path.join(path, "discovery.properties")
    if os.path.isfile(f):
      yield path
    elif os.path.isdir(path):
      for subPath in os.listdir(path):
        subPath = os.path.join(path, subPath)
        f = os.path.join(os.path.join(subPath, "discovery.properties"))
        if os.path.isfile(f):
          yield subPath
  
class RemoteEngine:
  def __init__(self, name, url):
    self.name = name
    self.url = url
  def __str__(self):
    return "[RemoteEngine name=%s url=%s]" % (self.name, self.url)

class LocalEngine:
  def __init__(self, path):
    if not os.path.isdir(path):
      raise "Not a directory: %s" % path
    self.path = path
    self.name = os.path.basename(path)
    self.refresh()
  def refresh(self):
    self.properties = readProperties(
      os.path.join(self.path, "discovery.properties"), "=")
    if self.properties is None:
      raise "Couldn't read properties file in %s" % self.path
    self.port = int(self.properties["port"])
    self.url = "http://localhost:%s" % self.port
    if self.properties.has_key("name"):
      self.name = self.properties["name"]
    try:
      self.state = readProperties(
        os.path.join(self.path, "discovery.state"), ":")
    except IOError:
      self.state = None
    pid = readFile(os.path.join(self.path, "discovery.pid"))
    if pid is not None:
      try:
        self.pid = int(pid)
      except ValueError:
        self.pid = None
    else:
      self.pid = None
  def isRunning(self):
    output = False
    if self.state is not None and self.state["state"] != "stopped":
      output = self.isProcessLive()
    return output
  def isProcessLive(self):
    output = False
    if self.pid is not None:
      try:
        os.kill(self.pid, 0)
        output = True
      except OSError:
        pass
    return output
  def __str__(self):
    output = "[LocalEngine port=%s name=%s" % (self.port, self.name)
    if self.pid is not None:
      output += " pid=%s" % self.pid
    output += "]"
    return output

def findEngine(name, engines):
  engine = None
  matches = filter(lambda e: e.name == name, engines)
  if len(matches) == 0:
    # No match by name, try by port
    try:
      port = int(name)
    except ValueError:
      port = None
    if port is not None:
      matches = filter(lambda e: e.port == port, engines)
  if len(matches) > 0:
    engine = matches[0]
  return engine

def munin_plugin(argv, monitors):
  parser = OptionParser("Usage: %prog [options] [config|autoconf]")
  parser.add_option("-r", "--remote-name", dest="remote_name", help="Name of remote engine")
  parser.add_option("-u", "--remote-url", dest="remote_url", help="URL of remote engine")
  parser.add_option("-l", "--local", dest="local", help="Directory of local engine")
  parser.add_option("-p", "--path", dest="path", help="Path to local engines")
  parser.add_option("-n", "--name", dest="name", help="Name of engine to use")
  parser.add_option("-m", "--monitor", dest="monitor", help="Name of monitor to use")
  parser.add_option("-E", "--list-engines", action="store_true", default=False, dest="list_engines", help="List available engines")
  parser.add_option("-M", "--list-monitors", action="store_true", default=False, dest="list_monitors", help="List available monitors")
  parser.add_option("-S", "--script-symlinks", action="store_true", default=False, dest="script_symlinks", help="Generate a script to create the appropriate symlinks for deployment")
  parser.add_option("-v", "--verbose", action="store_true", default=False, dest="verbose", help="Enable verbose output")
  (options, args) = parser.parse_args(argv)

  # Work around in munin where it passes us one argument which is the
  # emptry string.  
  if len(args) == 2 and len(args[1]) == 0:
    args = [args[0]]

  if len(args) > 2:
    print parser.format_help()
    raise SystemExit(1)

  if options.list_monitors:
    print "Local monitors:"
    for name in monitors.keys():
      monitor = monitors[name]
      if monitor[2]:
        print "  %s" % name
    print "Remote monitors:"
    for name in monitors.keys():
      monitor = monitors[name]
      if not monitor[2]:
        print "  %s" % name
    raise SystemExit(0)
  
  engines = []
  engine = None
  if options.remote_name is not None or options.remote_url is not None:
    name = options.remote_name
    if name is None:
      name = "RemoteEngine"
    engine = RemoteEngine(name, options.remote_url)
    engines.append(engine)
  elif options.local is not None:
    engine = LocalEngine(options.local)
    engines.append(engine)
  else:
    if options.path is not None:
      path = options.path
    else:
      path = os.environ.get('DISCOVERY_PATH', '')
    for dir in findEngineDirs(path):
      engine = LocalEngine(dir)
      engines.append(engine)
    engine = None

  if options.script_symlinks:
    target = args[0]
    basename = os.path.basename(target)
    if basename[-1] != '_':
      basename = basename + '_'
    for engine in engines:
      for monitor_name in monitors.keys():
        print "ln -s \"%s\" \"%s\"" % (target,
          "%s%s_%s" % (basename, monitor_name, engine.name))
    raise SystemExit(0)

  if options.name is not None:
    engine = findEngine(options.name, engines)
    if engine is None:
      print "No such engine: %s" % options.name
      raise SystemExit(1)

  if options.list_engines:
    print "Available engines:"
    for engine in engines:
      if hasattr(engine, 'path'):
        print "  name=%s port=%s path=%s" % (engine.name, engine.port, engine.path)
      else:
        print "  name=%s url=%s" % (engine.name, engine.url)
    liveEngines = filter(lambda e: hasattr(e, 'isRunning') and e.isRunning(), engines)
    if len(liveEngines) > 0:
      print "Live local engines:"
      for engine in liveEngines:
        print "  name=%s port=%s pid=%s" % (engine.name, engine.port, engine.pid)
        if options.verbose:
          print "    state:"
          for key,value in engine.state.items():
            print "      %s = %s" % (key, value)
          print "    properties:"
          for key,value in engine.properties.items():
            print "      %s = %s" % (key, value)
    raise SystemExit(0)

  monitor_name = None
  if options.monitor is not None:
    monitor_name = options.monitor
  else:
    # Try to extract from the program name
    tokens = args[0].split("_")
    if len(tokens) > 1:
      del tokens[0]
      name = tokens[-1]
      del tokens[-1]
      if len(tokens) > 0 and len(tokens[-1]) == 0:
        del tokens[-1]
      if len(tokens) > 0:
        monitor_name = "_".join(tokens)
      if len(name) == 0:
        print "Couldn't determine engine name from script name"
        raise SystemExit(1)
      engine = findEngine(name, engines)
      if engine is None:
        print "Couldn't find engine: %s" % name
        raise SystemExit(1)
  if monitor_name is None:
    print "Couldn't determine which monitor to use"
    raise SystemExit(1)
  try:
    (configFn, fetchFn, isLocal) = monitors[monitor_name]
  except KeyError:
    print "Unsupported monitor: %s" % monitor_name
    raise SystemExit(1)
 
  if len(args) == 1:
    if engine is None:
      print "You have to specify an engine from which to fetch"
      raise SystemExit(1)
    if isLocal:
      if not hasattr(engine, 'path'):
        print "The %s monitor requires a local engine" % monitor_name
        raise SystemError
      fetchFn(engine)
    else:
      fetchFn(engine.name, engine.url)
  elif args[1] == 'config':
    if isLocal:
      configFn(engine)
    else:
      configFn(engine.name, engine.url)
  elif args[1] == 'autoconf':
    print "yes"
    raise SystemExit(0)
  else:
    print parser.format_help()
    raise SystemExit(1)

def memory_config(name, url):
  print "graph_title memory stats for %s (%s)" % (name, url)
  print "graph_vlabel size (bytes)"
  print "graph_args --base 1024 -l 0"
  print "graph_category engine"
  print "freememory.label free memory"
  print "totalmemory.label total memory"
  print "maxmemory.label max memory"
  print "graph.info Amount of free, total, and max memory reported as " + \
        "reported by the engine, sampled every 5 minutes."

def memory_fetch(name, url):
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/info/runtime"))
  try:
    doc = xml.dom.minidom.parse(sock).documentElement
  finally:
    sock.close()
  print "freememory.value %s" % \
    doc.getElementsByTagName("freeMemory")[0].firstChild.data
  print "totalmemory.value %s" % \
    doc.getElementsByTagName("totalMemory")[0].firstChild.data
  print "maxmemory.value %s" % \
    doc.getElementsByTagName("maxMemory")[0].firstChild.data

def query_time_config(name, url):
  print "graph_title aggregate query time for %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "regular.label regular queries"
  print "regular.type DERIVE"
  print "regular.min 0"
  print "empty.label empty queries"
  print "empty.type DERIVE"
  print "empty.min 0"
  print "graph.info Time spent responding to queries (msec/sec)."

def query_time_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/query")
  if stats:
    print "regular.value %s" % stats.get("query.regular.time.sum", 0)
    print "empty.value %s" % stats.get("query.empty.time.sum", 0)

def query_num_config(name, url):
  print "graph_title number of queries for %s (%s)" % (name, url)
  print "graph_vlabel queries per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "regular.label regular queries"
  print "regular.type DERIVE"
  print "regular.min 0"
  print "graph.info Number of queries handled by engine."

def query_num_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/query")
  if stats:
    print "regular.value %s" % stats.get("query.regular.count", 0)

def xmlrpc_count_config(name, url):
  print "graph_title xmlrpc requests for %s (%s)" % (name, url)
  print "graph_vlabel queries per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "success.label successful queries"
  print "success.type DERIVE"
  print "success.min 0"
  print "invalid.label invalid queries"
  print "invalid.type DERIVE"
  print "invalid.min 0"
  print "failed.label failed queries"
  print "failed.type DERIVE"
  print "failed.min 0"
  print "graph.info Number of xmlrpc requests handled by engine, by outcome."

def xmlrpc_count_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/xmlrpc")
  if stats:
    print "success.value %s" % stats.get("xmlrpc.success.count", 0)
    print "invalid.value %s" % stats.get("xmlrpc.invalid.count", 0)
    print "failed.value %s" % stats.get("xmlrpc.failed.count", 0)

def xmlrpc_time_config(name, url):
  print "graph_title xmlrpc time for %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "success.label successful queries"
  print "success.type DERIVE"
  print "success.min 0"
  print "invalid.label invalid queries"
  print "invalid.type DERIVE"
  print "invalid.min 0"
  print "failed.label failed queries"
  print "failed.type DERIVE"
  print "failed.min 0"
  print "graph.info Time spent handling xmlrpc requests, by outcome."

def xmlrpc_time_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/xmlrpc")
  if stats:
    print "success.value %s" % stats.get("xmlrpc.success.sum", 0)
    print "invalid.value %s" % stats.get("xmlrpc.invalid.sum", 0)
    print "failed.value %s" % stats.get("xmlrpc.failed.sum", 0)

def checkpoint_time_config(name, url):
  print "graph_title time spent checkpointing for %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "time.label time checkpointing"
  print "time.type DERIVE"
  print "time.min 0"
  print "graph.info Time spent checkpointing."

def checkpoint_time_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/checkpoint")
  if stats:
    print "time.value %s" % stats.get("checkpoint.time.sum", 0)

def checkpoint_count_config(name, url):
  print "graph_title number of checkpoints for %s (%s)" % (name, url)
  print "graph_vlabel checkpoints per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "count.label number of checkpoints"
  print "count.type DERIVE"
  print "count.min 0"
  print "graph.info Number of checkpoints."

def checkpoint_count_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/checkpoint")
  if stats:
    print "count.value %s" % stats.get("checkpoint.time.count", 0)

def http_count_config(name, url):
  print "graph_title number of http requests to %s (%s)" % (name, url)
  print "graph_vlabel requests per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "count.label number of requests"
  print "count.type DERIVE"
  print "count.min 0"
  print "graph.info Number of http requests served by the engine."

def http_count_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/http")
  if stats:
    print "count.value %s" % stats.get("http.time.count", 0)

def http_time_config(name, url):
  print "graph_title time to serve http requests to %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "time.label time"
  print "time.type DERIVE"
  print "time.min 0"
  print "graph.info Time to serve all http requests to the engine."

def http_time_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/http")
  if stats:
    print "time.value %s" % stats.get("http.time.sum", 0)

def changeset_count_config(name, url):
  print "graph_title changesets processed by type: %s (%s)" % (name, url)
  print "graph_vlabel changesets per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "reset.label reset"
  print "reset.type DERIVE"
  print "reset.min 0"
  print "delta.label delta"
  print "delta.type DERIVE"
  print "delta.min 0"
  print "snapshot.label snapshot"
  print "snapshot.type DERIVE"
  print "snapshot.min 0"
  print "checkpoint.label checkpoint"
  print "checkpoint.type DERIVE"
  print "checkpoint.min 0"
  print "graph.info Number of changesets processed by the engine, by type."

def changeset_count_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/changeset")
  if stats:
    print "reset.value %s" % stats.get("changeset.reset.size.count", 0)
    print "delta.value %s" % stats.get("changeset.delta.size.count", 0)
    print "snapshot.value %s" % stats.get("changeset.snapshot.size.count", 0)
    print "checkpoint.value %s" % stats.get("changeset.checkpoint.size.count", 0)

def changeset_size_config(name, url):
  print "graph_title size (uncompressed) of processed changesets by type: %s (%s)" % (name, url)
  print "graph_vlabel bytes per ${graph_period}"
  print "graph_args --base 1024 -l 0"
  print "graph_category engine"
  print "reset.label reset"
  print "reset.type DERIVE"
  print "reset.min 0"
  print "delta.label delta"
  print "delta.type DERIVE"
  print "delta.min 0"
  print "snapshot.label snapshot"
  print "snapshot.type DERIVE"
  print "snapshot.min 0"
  print "checkpoint.label checkpoint"
  print "checkpoint.type DERIVE"
  print "checkpoint.min 0"
  print "graph.info Size in bytes of uncompressed changesets processed by the engine, by type."

def changeset_size_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/changeset")
  if stats:
    print "reset.value %s" % stats.get("changeset.reset.size.sum", 0)
    print "delta.value %s" % stats.get("changeset.delta.size.sum", 0)
    print "snapshot.value %s" % stats.get("changeset.snapshot.size.sum", 0)
    print "checkpoint.value %s" % stats.get("changeset.checkpoint.size.sum", 0)

def changeset_items_config(name, url):
  print "graph_title items in changesets by type: %s (%s)" % (name, url)
  print "graph_vlabel changes per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "created.label created items"
  print "created.type DERIVE"
  print "created.min 0"
  print "modified.label modified items"
  print "modified.type DERIVE"
  print "modified.min 0"
  print "deleted.label deleted items"
  print "deleted.type DERIVE"
  print "deleted.min 0"
  print "graph.info Number of items changed in changesets, by type."

def changeset_items_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/changeset.apply")
  if stats:
    print "created.value %s" % stats.get("changeset.apply.created.sum", 0)
    print "modified.value %s" % stats.get("changeset.apply.modified.sum", 0)
    print "deleted.value %s" % stats.get("deleted.apply.created.sum", 0)

def items_count_config(name, url):
  print "graph_title items in dataset for %s (%s)" % (name, url)
  print "graph_vlabel number of items"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "indexed.label items indexed in current partition"
  print "total.label total items in dataset"
  print "graph.info Number of items indexed in the current partition, and in the entire dataset."

def items_count_fetch(name, url):
  stats = readStats(url, "ws/statistics/fetch/index item")
  if stats:
    print "indexed.value %s" % stats.get("index.items", 0)
    print "total.value %s" % stats.get("item.count", 0)

def dir_size_config(engine):
  print "graph_title dir size for %s (%s)" % (engine.name, engine.port)
  print "graph_vlabel size (bytes)"
  print "items.label db/items"
  print "lucene.label db/lucene"
  print "graph_args --base 1024 -l 0"
  print "graph_category engine"
  print "graph.info Disk usage for the db/items and db/lucene directories " + \
        "taken every 5 minutes."
  print "items.label Disk usage for db/items"
  print "lucene.label Disk usage for db/lucene"

def dir_size_fetch(engine):
  db_path = os.path.join(engine.path, "db")
  items_path = os.path.join(db_path, "items")
  lucene_path = os.path.join(db_path, "lucene")
  if os.path.isdir(items_path):
    print "items.value %s" % getDirSize(items_path)
  if os.path.isdir(lucene_path):
    print "lucene.value %s" % getDirSize(lucene_path)

def jstat_heap_config(engine):
  print "graph_title Heap Usage for %s (%s)" % (engine.name, engine.port)
  print "graph_args --base 1024 -l 0"
  print "graph_vlabel Heap Usage(Bytes)"
  print "graph_info Heap Usage"
  print "graph_category engine"
  print "Eden_Used.label Eden_Used"
  print "Eden_Free.label Eden_Free"
  print "Survivor0_Used.label Survivor0_Used"
  print "Survivor0_Free.label Survivor0_Free"
  print "Survivor1_Used.label Survivor1_Used"
  print "Survivor1_Free.label Survivor1_Free"
  print "Old_Used.label Old_Used"
  print "Old_Free.label Old_Free"
  print "Permanent_Used.label Permanent_Used"
  print "Permanent_Free.label Permanent_Free"
  print "Eden_Used.draw AREA"
  print "Eden_Free.draw STACK"
  print "Survivor0_Used.draw STACK"
  print "Survivor0_Free.draw STACK"
  print "Survivor1_Used.draw STACK"
  print "Survivor1_Free.draw STACK"
  print "Old_Used.draw STACK"
  print "Old_Free.draw STACK"
  print "Permanent_Used.draw STACK"
  print "Permanent_Free.draw STACK"

def jstat_heap_fetch(engine):
  out = commands.getoutput("jstat -gc %s" % engine.pid)
  lines = out.splitlines()
  names = filter(lambda x: len(x) > 0, lines[0].split(" "))
  values = filter(lambda x: len(x) > 0, lines[1].split(" "))
  jstat = {}
  i = 0
  while i < len(names):
    jstat[names[i]] = float(values[i])
    i += 1
  del out, lines, names, values, i
  from math import ceil
  print "Eden_Used.value %s" % int(ceil( \
    jstat['EU'] * 1024))
  print "Eden_Free.value %s" % int(ceil( \
    (jstat['EC'] - jstat['EU']) * 1024))
  print "Survivor0_Used.value %s" % int(ceil( \
    jstat['S0U'] * 1024))
  print "Survivor0_Free.value %s" % int(ceil( \
    (jstat['S0C'] - jstat['S0U']) * 1024))
  print "Survivor1_Used.value %s" % int(ceil(\
    jstat['S1U'] * 1024))
  print "Survivor1_Free.value %s" % int(ceil( \
    (jstat['S1C'] - jstat['S1U']) * 1024))
  print "Old_Used.value %s" % int(ceil( \
    jstat['OU'] * 1024))
  print "Old_Free.value %s" % int(ceil( \
    (jstat['OC'] - jstat['OU']) * 1024))
  print "Permanent_Used.value %s" % int(ceil( \
    jstat['PU'] * 1024))
  print "Permanent_Free.value %s" % int(ceil( \
    (jstat['PC'] - jstat['PU']) * 1024))

def bad_queries_config(engine):
  print "graph_title Problem queries for %s (%s)" % (engine.name, engine.port)
  print "graph_vlabel queries per ${graph_period}"
  print "graph_period minute"
  print "slow.label slow queries per minute"
  print "slow.type DERIVE"
  print "slow.min 0"
  print "slow.warning 20"
  print "slow.critical 50"
  print "failed.label failed queries per minute"
  print "failed.type DERIVE"
  print "failed.min 0"
  print "invalid.label invalid queries per minute" 
  print "invalid.type DERIVE"
  print "invalid.min 0"
  print "luceneslow.label retried lucene queries per minute" 
  print "luceneslow.type DERIVE"
  print "luceneslow.min 0"
  print "lucenefailed.label failed lucene queries per minute" 
  print "lucenefailed.type DERIVE"
  print "lucenefailed.min 0"
  print "oom.label out of memory errors per minute" 
  print "oom.type DERIVE"
  print "oom.min 0"
  print "graph_args -l 0"
  print "graph_category engine"
  print "graph.info Count of queries slow enough to be logged to the " + \
        "aggregator's log file. Collected every 5 minutes."

def bad_queries_fetch(engine):
  log_path = os.path.join(os.path.join(engine.path, "log"), "discovery.log")
  if os.path.exists(log_path):
    aggregatingRe = re.compile( \
      r"\[com\.t11e\.progress\] Aggregating query .*\[done\]")
    rpcRe = re.compile( \
      r"\[com\.t11e\.progress\] POST /RPC2 .*\[done\]")
    failedRe = re.compile( \
      r"Query failed with exception")
    invalidRe = re.compile( \
      r"Controller returned error 400 when handling POST request to /RPC2")
    luceneSlowRe = re.compile ( \
      r"Lucene search required")
    luceneFailedRe = re.compile ( \
      r"Lucene search failed")
    oomRe = re.compile ( \
      r"java\.lang\.OutOfMemoryError")
    aggregatingCount = 0
    rpcCount = 0
    failedCount = 0
    invalidCount = 0
    luceneSlowCount = 0
    luceneFailedCount = 0
    oomCount = 0
    f = open(log_path)
    try:      
      for line in f:
        if aggregatingRe.search(line) is not None:
          aggregatingCount += 1
        if rpcRe.search(line) is not None:
          rpcCount += 1
        if failedRe.search(line) is not None:
          failedCount += 1
        if invalidRe.search(line) is not None:
          invalidCount += 1
        if luceneSlowRe.search(line) is not None:
          luceneSlowCount = 0
        if luceneFailedRe.search(line) is not None:
          luceneFailedCount = 0
        if oomRe.search(line) is not None:
          oomCount = 0
    finally:
      f.close()
    if aggregatingCount > 0:
      print "slow.value %s" % aggregatingCount
    else:
      print "slow.value %s" % rpcCount
    print "failed.value %s" % failedCount
    print "invalid.value %s" % invalidCount
    print "luceneslow.value %s" % luceneSlowCount
    print "lucenefailed.value %s" % luceneFailedCount
    print "oom.value %s" % oomCount

MONITORS = {
  "memory": (memory_config, memory_fetch, False),
  "dir_size": (dir_size_config, dir_size_fetch, True),
  "jstat_heap": (jstat_heap_config, jstat_heap_fetch, True),
  "bad_queries": (bad_queries_config, bad_queries_fetch, True),
  "query_time": (query_time_config, query_time_fetch, False),
  "query_num": (query_num_config, query_num_fetch, False),
  "xmlrpc_count": (xmlrpc_count_config, xmlrpc_count_fetch, False),
  "xmlrpc_time": (xmlrpc_time_config, xmlrpc_time_fetch, False),
  "checkpoint_time": (checkpoint_time_config, checkpoint_time_fetch, False),
  "checkpoint_count": (checkpoint_count_config, checkpoint_count_fetch, False),
  "http_count": (http_count_config, http_count_fetch, False),
  "http_time": (http_time_config, http_time_fetch, False),
  "changeset_count": (changeset_count_config, changeset_count_fetch, False),
  "changeset_size": (changeset_size_config, changeset_size_fetch, False),
  "changeset_items": (changeset_items_config, changeset_items_fetch, False),
  "items_count": (items_count_config, items_count_fetch, False),
}

if __name__ == '__main__':
  munin_plugin(sys.argv, MONITORS)
