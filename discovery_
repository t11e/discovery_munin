#!/usr/bin/env python
#%# family=auto
#%# capabilities=autoconf suggest

import commands
import math
import os
import re
import sys
import urllib
import urlparse
import xml.dom.minidom

def getDirSize(path):
  size = 0
  for f in os.listdir(path):
    if os.path.isdir(os.path.join(path, f)):
      size += getDirSize(os.path.join(path, f))
    else:
      size += os.path.getsize(os.path.join(path, f))
  return size

def readFile(path):
  output = None
  try:
    f = open(path)
  except IOError:
    f = None
  if f is not None:
    try:
      output = f.read()
    finally:
      f.close()
  return output

def readProperties(path, separator):
  output = None
  try:
    f = open(path)
  except IOError:
    f = None
  if f is not None:
    try:
      output = {}
      for line in f:
        line = line.strip()
        if len(line) == 0 or line.startswith("#"): continue
        try:
          key, value = map(lambda s: s.strip(), line.split(separator, 1))
          output[key] = value
        except ValueError:
          print "Ignoring line: %s" % line
    finally:
      f.close()
  return output

def findEnginePaths(searchPath):
  for path in searchPath.split(os.path.pathsep):
    if len(path) == 0: continue
    f = os.path.join(path, "discovery.properties")
    if os.path.isfile(f):
      yield path
    elif os.path.isdir(path):
      for subPath in os.listdir(path):
        subPath = os.path.join(path, subPath)
        f = os.path.join(os.path.join(subPath, "discovery.properties"))
        if os.path.isfile(f):
          yield subPath
  
class LocalEngine:
  def __init__(self, path):
    if not os.path.isdir(path):
      raise "Not a directory: %s" % path
    self.path = path
    self.name = os.path.basename(path)
    self.refresh()
  def refresh(self):
    self.properties = readProperties(
      os.path.join(self.path, "discovery.properties"), "=")
    if self.properties is None:
      raise "Couldn't read properties file in %s" % self.path
    self.port = int(self.properties["port"])
    self.url = "http://localhost:%s" % self.port
    if self.properties.has_key("name"):
      self.name = self.properties["name"]
    try:
      self.state = readProperties(
        os.path.join(self.path, "discovery.state"), ":")
    except IOError:
      self.state = None
    pid = readFile(os.path.join(self.path, "discovery.pid"))
    if pid is not None:
      try:
        self.pid = int(pid)
      except ValueError:
        self.pid = None
    else:
      self.pid = None
  def isRunning(self):
    output = False
    if self.state is not None and self.state["state"] != "stopped":
      output = self.isProcessLive()
    return output
  def isProcessLive(self):
    output = False
    if self.pid is not None:
      try:
        os.kill(self.pid, 0)
        output = True
      except OSError:
        pass
    return output
  def __str__(self):
    output = "[LocalEngine port=%s name=%s" % (self.port, self.name)
    if self.pid is not None:
      output += " pid=%s" % self.pid
    output += "]"
    return output

def extractMonitorAndPort(arg):
  monitor = None
  port = None
  tokens = arg.split("_", 1)
  if len(tokens) == 2 and len(tokens[1]) != 0:
    program = tokens[0]
    rest = tokens[1]
    tokens = rest.rsplit("_", 1)
    if len(tokens) == 2:
      if len(tokens[1]) == 0:
        monitor = tokens[0]
      else:
        try:
          port = int(tokens[1])
          monitor = tokens[0]
        except ValueError:
          pass      
      if port is None:
        monitor = rest
  return (monitor, port)

def getMonitorAndPort(arg, monitors):
  configFn = None
  fetchFn = None
  isLocal = True
  port = None
  monitor = None
  tokens = arg.split("_")
  if len(tokens) > 1:
    del tokens[0]
    try:
      port = int(tokens[-1])
      del tokens[-1]
    except ValueError:
      pass
    if len(tokens[-1]) == 0:
      del tokens[-1]
    if len(tokens) > 0:
      monitor = "_".join(tokens)
  if monitor is not None:
    try:
      (configFn, fetchFn, isLocal) = monitors[monitor]
    except KeyError:
      raise "Unknown monitor type: %s " % monitor
  return (port, configFn, fetchFn, isLocal)

def findEngine(port):
  engine = None
  engines = {}
  path = os.environ.get('DISCOVERY_DIR', None)
  if path is not None and os.path.isfile(os.path.join(path, \
                                                      "discovery.properties")):
    port = None
    engine = LocalEngine(path)
    engines[engine.port] = engine
  if engine is None:
    paths = os.environ.get('DISCOVERY_PATH', '')
    for path in findEnginePaths(paths):
      engine = LocalEngine(path)
      if not engine.port in engines:
        engines[engine.port] = engine
    if port is not None:
      engine = engines[port]
    else:
      engine = None
  return (engine, engines)
  
def munin_plugin(argv, monitors):
  usage_message = \
    "Usage: %s [config|suggest|engines]" % os.path.basename(argv[0])
  if len(argv) < 1 or len(argv) > 2:
    print usage_message
    raise SystemExit(1)

  port, configFn, fetchFn, isLocal = getMonitorAndPort(argv[0], monitors)
  engine, engines = findEngine(port)

  if len(argv) == 1 or (len(argv) == 2 and len(argv[1]) == 0):
    if fetchFn is None or engine is None:
      print "Can't derive engine instance from program name."
      raise SystemExit(1)
    if isLocal:
      fetchFn(engine)
    else:
      fetchFn(engine.name, engine.url)
  elif argv[1] == 'config':
    if configFn is None:
      print "Can't derive monitor type from program name."
      raise SystemExit(1)
    if isLocal:
      configFn(engine)
    else:
      configFn(engine.name, engine.url)
  elif argv[1] == 'suggest':
    if configFn is None or fetchFn is None:
      for monitor in monitors.keys():
        print monitor
      pass
    else:
      for port in engines.keys():
        print port
  elif argv[1] == 'autoconf':
    print "yes"
    raise SystemExit(0)
  elif argv[1] == "engines":
    print "Available engines:"
    for port, engine in engines.items():
      print "  port=%s name=%s path=%s" % (engine.port, engine.name, engine.path)
    print "Live engines:"
    for port, engine in engines.items():
      if engine.isRunning():
        print "  port=%s name=%s pid=%s" % (engine.port, engine.name, engine.pid)
        print "    state:"
        for key,value in engine.state.items():
          print "      %s = %s" % (key, value)
        print "    properties:"
        for key,value in engine.properties.items():
          print "      %s = %s" % (key, value)
  else:
    print usage_message
    raise SystemExit(1)

def memory_config(name, url):
  print "graph_title memory stats for %s (%s)" % (name, url)
  print "graph_vlabel size (bytes)"
  print "graph_args --base 1024 -l 0"
  print "graph_category engine"
  print "freememory.label free memory"
  print "totalmemory.label total memory"
  print "maxmemory.label max memory"
  print "graph.info Amount of free, total, and max memory reported as " + \
        "reported by the engine, sampled every 5 minutes."

def memory_fetch(name, url):
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/info/runtime"))
  try:
    doc = xml.dom.minidom.parse(sock).documentElement
  finally:
    sock.close()
  print "freememory.value %s" % \
    doc.getElementsByTagName("freeMemory")[0].firstChild.data
  print "totalmemory.value %s" % \
    doc.getElementsByTagName("totalMemory")[0].firstChild.data
  print "maxmemory.value %s" % \
    doc.getElementsByTagName("maxMemory")[0].firstChild.data

def query_time_config(name, url):
  print "graph_title aggregate query time for %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "regular.label regular queries"
  print "regular.type derive"
  print "regular.min 0"
  print "empty.label empty queries"
  print "empty.type derive"
  print "empty.min 0"
  print "graph.info Time spent responding to queries (msec/sec)."

def query_time_fetch(name, url):
  regularSum = 0
  emptySum = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/query"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "query.regular.time.sum:":
        regularSum = value
      elif label == "query.empty.time.sum:":
        emptySum = value
  finally:
    sock.close()
  print "regular.value %s" % regularSum
  print "empty.value %s" % emptySum

def query_num_config(name, url):
  print "graph_title number of queries for %s (%s)" % (name, url)
  print "graph_vlabel queries per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "regular.label regular queries"
  print "regular.type derive"
  print "regular.min 0"
  print "graph.info Number of queries handled by engine."

def query_num_fetch(name, url):
  queries = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/query"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "query.regular.count:":
        queries = value
  finally:
    sock.close()
  print "regular.value %s" % queries

def xmlrpc_count_config(name, url):
  print "graph_title xmlrpc requests for %s (%s)" % (name, url)
  print "graph_vlabel queries per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "success.label successful queries"
  print "success.type derive"
  print "success.min 0"
  print "invalid.label invalid queries"
  print "invalid.type derive"
  print "invalid.min 0"
  print "failed.label failed queries"
  print "failed.type derive"
  print "failed.min 0"
  print "graph.info Number of xmlrpc requests handled by engine, by outcome."

def xmlrpc_count_fetch(name, url):
  success = 0
  invalid = 0
  failed = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/xmlrpc"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "xmlrpc.success.count:":
        success = value
      elif label == "xmlrpc.invalid.count:":
        invalid = value
      elif label == "xmlrpc.failed.count:":
        failed = value
  finally:
    sock.close()
  print "success.value %s" % success
  print "invalid.value %s" % invalid
  print "failed.value %s" % failed

def xmlrpc_time_config(name, url):
  print "graph_title xmlrpc time for %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "success.label successful queries"
  print "success.type derive"
  print "success.min 0"
  print "invalid.label invalid queries"
  print "invalid.type derive"
  print "invalid.min 0"
  print "failed.label failed queries"
  print "failed.type derive"
  print "failed.min 0"
  print "graph.info Time spent handling xmlrpc requests, by outcome."

def xmlrpc_time_fetch(name, url):
  success = 0
  invalid = 0
  failed = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/xmlrpc"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "xmlrpc.success.sum:":
        success = value
      elif label == "xmlrpc.invalid.sum:":
        invalid = value
      elif label == "xmlrpc.failed.sum:":
        failed = value
  finally:
    sock.close()
  print "success.value %s" % success
  print "invalid.value %s" % invalid
  print "failed.value %s" % failed

def checkpoint_time_config(name, url):
  print "graph_title time spent checkpointing for %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "time.label time checkpointing"
  print "time.type derive"
  print "time.min 0"
  print "graph.info Time spent checkpointing."

def checkpoint_time_fetch(name, url):
  time = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/checkpoint"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "checkpoint.time.sum:":
        time = value
  finally:
    sock.close()
  print "time.value %s" % time

def checkpoint_count_config(name, url):
  print "graph_title number of checkpoints for %s (%s)" % (name, url)
  print "graph_vlabel checkpoints per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "count.label number of checkpoints"
  print "count.type derive"
  print "count.min 0"
  print "graph.info Number of checkpoints."

def checkpoint_count_fetch(name, url):
  count = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/checkpoint"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "checkpoint.time.count:":
        count = value
  finally:
    sock.close()
  print "count.value %s" % count

def http_count_config(name, url):
  print "graph_title number of http requests to %s (%s)" % (name, url)
  print "graph_vlabel requests per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "count.label number of requests"
  print "count.type derive"
  print "count.min 0"
  print "graph.info Number of http requests served by the engine."

def http_count_fetch(name, url):
  count = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/http"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "http.time.count:":
        count = value
  finally:
    sock.close()
  print "count.value %s" % count

def http_time_config(name, url):
  print "graph_title time to serve http requests to %s (%s)" % (name, url)
  print "graph_vlabel time (msec per ${graph_period})"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "time.label time"
  print "time.type derive"
  print "time.min 0"
  print "graph.info Time to serve all http requests to the engine."

def http_time_fetch(name, url):
  time = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/http"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "http.time.sum:":
        time = value
  finally:
    sock.close()
  print "time.value %s" % time

def changeset_count_config(name, url):
  print "graph_title changesets processed by type: %s (%s)" % (name, url)
  print "graph_vlabel changesets per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "reset.label reset"
  print "reset.type derive"
  print "reset.min 0"
  print "delta.label delta"
  print "delta.type derive"
  print "delta.min 0"
  print "snapshot.label snapshot"
  print "snapshot.type derive"
  print "snapshot.min 0"
  print "checkpoint.label checkpoint"
  print "checkpoint.type derive"
  print "checkpoint.min 0"
  print "graph.info Number of changesets processed by the engine, by type."

def changeset_count_fetch(name, url):
  resets = 0
  deltas = 0
  snapshots = 0
  checkpoints = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/changeset"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "changeset.reset.size.count:":
        resets = value
      elif label == "changeset.delta.size.count:":
        deltas = value
      elif label == "changeset.snapshot.size.count:":
        snapshots = value
      elif label == "changeset.checkpoint.size.count:":
        checkpoints = value
  finally:
    sock.close()
  print "reset.value %s" % resets
  print "delta.value %s" % deltas
  print "snapshot.value %s" % snapshots
  print "checkpoint.value %s" % checkpoints

def changeset_size_config(name, url):
  print "graph_title size (uncompressed) of processed changesets by type: %s (%s)" % (name, url)
  print "graph_vlabel bytes per ${graph_period}"
  print "graph_args --base 1024 -l 0"
  print "graph_category engine"
  print "reset.label reset"
  print "reset.type derive"
  print "reset.min 0"
  print "delta.label delta"
  print "delta.type derive"
  print "delta.min 0"
  print "snapshot.label snapshot"
  print "snapshot.type derive"
  print "snapshot.min 0"
  print "checkpoint.label checkpoint"
  print "checkpoint.type derive"
  print "checkpoint.min 0"
  print "graph.info Size in bytes of uncompressed changesets processed by the engine, by type."

def changeset_size_fetch(name, url):
  resets = 0
  deltas = 0
  snapshots = 0
  checkpoints = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/changeset"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "changeset.reset.size.sum:":
        resets = value
      if label == "changeset.delta.size.sum:":
        deltas = value
      if label == "changeset.snapshot.size.sum:":
        snapshots = value
      if label == "changeset.checkpoint.size.sum:":
        checkpoints = value
  finally:
    sock.close()
  print "reset.value %s" % resets
  print "delta.value %s" % deltas
  print "snapshot.value %s" % snapshots
  print "checkpoint.value %s" % checkpoints

def changeset_items_config(name, url):
  print "graph_title items in changesets by type: %s (%s)" % (name, url)
  print "graph_vlabel changes per ${graph_period}"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "created.label created items"
  print "created.type derive"
  print "created.min 0"
  print "modified.label modified items"
  print "modified.type derive"
  print "modified.min 0"
  print "deleted.label deleted items"
  print "deleted.type derive"
  print "deleted.min 0"
  print "graph.info Number of items changed in changesets, by type."

def changeset_items_fetch(name, url):
  created = 0
  modified = 0
  deleted = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/changeset"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "changeset.apply.created.sum:":
        created = value
      elif label == "changeset.apply.modified.sum:":
        modified = value
      elif label == "changeset.apply.deleted.sum:":
        deleted = value
  finally:
    sock.close()
  print "created.value %s" % created
  print "modified.value %s" % modified
  print "deleted.value %s" % deleted

def items_count_config(name, url):
  print "graph_title items in dataset for %s (%s)" % (name, url)
  print "graph_vlabel number of items"
  print "graph_args --base 1000 -l 0"
  print "graph_category engine"
  print "indexed.label items indexed in current partition"
  print "total.label total items in dataset"
  print "graph.info Number of items indexed in the current partition, and in the entire dataset."

def items_count_fetch(name, url):
  indexed = 0
  total = 0
  sock = urllib.urlopen(urlparse.urljoin(url, "ws/statistics/fetch/index item"))
  try:
    for line in sock.readlines():
      (label, value) = line.split()
      if label == "index.items:":
        created = value
      elif label == "item.count:":
        total = value
  finally:
    sock.close()
  print "indexed.value %s" % indexed
  print "total.value %s" % total

def dir_size_config(engine):
  print "graph_title dir size for %s (%s)" % (engine.name, engine.port)
  print "graph_vlabel size (bytes)"
  print "items.label db/items"
  print "lucene.label db/lucene"
  print "graph_args --base 1024 -l 0"
  print "graph_category engine"
  print "graph.info Disk usage for the db/items and db/lucene directories " + \
        "taken every 5 minutes."
  print "items.label Disk usage for db/items"
  print "lucene.label Disk usage for db/lucene"

def dir_size_fetch(engine):
  db_path = os.path.join(engine.path, "db")
  items_path = os.path.join(db_path, "items")
  lucene_path = os.path.join(db_path, "lucene")
  if os.path.isdir(items_path):
    print "items.value %s" % getDirSize(items_path)
  if os.path.isdir(lucene_path):
    print "lucene.value %s" % getDirSize(lucene_path)

def jstat_heap_config(engine):
  print "graph_title Heap Usage for %s (%s)" % (engine.name, engine.port)
  print "graph_args --base 1024 -l 0"
  print "graph_vlabel Heap Usage(Bytes)"
  print "graph_info Heap Usage"
  print "graph_category engine"
  print "Eden_Used.label Eden_Used"
  print "Eden_Free.label Eden_Free"
  print "Survivor0_Used.label Survivor0_Used"
  print "Survivor0_Free.label Survivor0_Free"
  print "Survivor1_Used.label Survivor1_Used"
  print "Survivor1_Free.label Survivor1_Free"
  print "Old_Used.label Old_Used"
  print "Old_Free.label Old_Free"
  print "Permanent_Used.label Permanent_Used"
  print "Permanent_Free.label Permanent_Free"
  print "Eden_Used.draw AREA"
  print "Eden_Free.draw STACK"
  print "Survivor0_Used.draw STACK"
  print "Survivor0_Free.draw STACK"
  print "Survivor1_Used.draw STACK"
  print "Survivor1_Free.draw STACK"
  print "Old_Used.draw STACK"
  print "Old_Free.draw STACK"
  print "Permanent_Used.draw STACK"
  print "Permanent_Free.draw STACK"

def jstat_heap_fetch(engine):
  out = commands.getoutput("jstat -gc %s" % engine.pid)
  lines = out.splitlines()
  names = filter(lambda x: len(x) > 0, lines[0].split(" "))
  values = filter(lambda x: len(x) > 0, lines[1].split(" "))
  jstat = {}
  i = 0
  while i < len(names):
    jstat[names[i]] = float(values[i])
    i += 1
  del out, lines, names, values, i
  from math import ceil
  print "Eden_Used.value %s" % int(ceil( \
    jstat['EU'] * 1024))
  print "Eden_Free.value %s" % int(ceil( \
    (jstat['EC'] - jstat['EU']) * 1024))
  print "Survivor0_Used.value %s" % int(ceil( \
    jstat['S0U'] * 1024))
  print "Survivor0_Free.value %s" % int(ceil( \
    (jstat['S0C'] - jstat['S0U']) * 1024))
  print "Survivor1_Used.value %s" % int(ceil(\
    jstat['S1U'] * 1024))
  print "Survivor1_Free.value %s" % int(ceil( \
    (jstat['S1C'] - jstat['S1U']) * 1024))
  print "Old_Used.value %s" % int(ceil( \
    jstat['OU'] * 1024))
  print "Old_Free.value %s" % int(ceil( \
    (jstat['OC'] - jstat['OU']) * 1024))
  print "Permanent_Used.value %s" % int(ceil( \
    jstat['PU'] * 1024))
  print "Permanent_Free.value %s" % int(ceil( \
    (jstat['PC'] - jstat['PU']) * 1024))

def bad_queries_config(engine):
  print "graph_title Problem queries for %s (%s)" % (engine.name, engine.port)
  print "graph_vlabel queries per ${graph_period}"
  print "graph_period minute"
  print "slow.label slow queries per minute"
  print "slow.type DERIVE"
  print "slow.min 0"
  print "slow.warning 20"
  print "slow.critical 50"
  print "failed.label failed queries per minute"
  print "failed.type DERIVE"
  print "failed.min 0"
  print "invalid.label invalid queries per minute" 
  print "invalid.type DERIVE"
  print "invalid.min 0"
  print "luceneslow.label retried lucene queries per minute" 
  print "luceneslow.type DERIVE"
  print "luceneslow.min 0"
  print "lucenefailed.label failed lucene queries per minute" 
  print "lucenefailed.type DERIVE"
  print "lucenefailed.min 0"
  print "oom.label out of memory errors per minute" 
  print "oom.type DERIVE"
  print "oom.min 0"
  print "graph_args -l 0"
  print "graph_category engine"
  print "graph.info Count of queries slow enough to be logged to the " + \
        "aggregator's log file. Collected every 5 minutes."

def bad_queries_fetch(engine):
  log_path = os.path.join(os.path.join(engine.path, "log"), "discovery.log")
  if os.path.exists(log_path):
    aggregatingRe = re.compile( \
      r"\[com\.t11e\.progress\] Aggregating query .*\[done\]")
    rpcRe = re.compile( \
      r"\[com\.t11e\.progress\] POST /RPC2 .*\[done\]")
    failedRe = re.compile( \
      r"Query failed with exception")
    invalidRe = re.compile( \
      r"Controller returned error 400 when handling POST request to /RPC2")
    luceneSlowRe = re.compile ( \
      r"Lucene search required")
    luceneFailedRe = re.compile ( \
      r"Lucene search failed")
    oomRe = re.compile ( \
      r"java\.lang\.OutOfMemoryError")
    aggregatingCount = 0
    rpcCount = 0
    failedCount = 0
    invalidCount = 0
    luceneSlowCount = 0
    luceneFailedCount = 0
    oomCount = 0
    f = open(log_path)
    try:      
      for line in f:
        if aggregatingRe.search(line) is not None:
          aggregatingCount += 1
        if rpcRe.search(line) is not None:
          rpcCount += 1
        if failedRe.search(line) is not None:
          failedCount += 1
        if invalidRe.search(line) is not None:
          invalidCount += 1
        if luceneSlowRe.search(line) is not None:
          luceneSlowCount = 0
        if luceneFailedRe.search(line) is not None:
          luceneFailedCount = 0
        if oomRe.search(line) is not None:
          oomCount = 0
    finally:
      f.close()
    if aggregatingCount > 0:
      print "slow.value %s" % aggregatingCount
    else:
      print "slow.value %s" % rpcCount
    print "failed.value %s" % failedCount
    print "invalid.value %s" % invalidCount
    print "luceneslow.value %s" % luceneSlowCount
    print "lucenefailed.value %s" % luceneFailedCount
    print "oom.value %s" % oomCount

MONITORS = {
  "memory": (memory_config, memory_fetch, False),
  "dir_size": (dir_size_config, dir_size_fetch, True),
  "jstat_heap": (jstat_heap_config, jstat_heap_fetch, True),
  "bad_queries": (bad_queries_config, bad_queries_fetch, True),
  "query_time": (query_time_config, query_time_fetch, False),
  "query_num": (query_num_config, query_num_fetch, False),
  "xmlrpc_count": (xmlrpc_count_config, xmlrpc_count_fetch, False),
  "xmlrpc_time": (xmlrpc_time_config, xmlrpc_time_fetch, False),
  "checkpoint_time": (checkpoint_time_config, checkpoint_time_fetch, False),
  "checkpoint_count": (checkpoint_count_config, checkpoint_count_fetch, False),
  "http_count": (http_count_config, http_count_fetch, False),
  "http_time": (http_time_config, http_time_fetch, False),
  "changeset_count": (changeset_count_config, changeset_count_fetch, False),
  "changeset_size": (changeset_size_config, changeset_size_fetch, False),
  "changeset_items": (changeset_items_config, changeset_items_fetch, False),
  "items_count": (items_count_config, items_count_fetch, False),
}

if __name__ == '__main__':
  munin_plugin(sys.argv, MONITORS)
